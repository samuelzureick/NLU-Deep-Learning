{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aFJVFnGExKz",
        "outputId": "7a8128cf-928f-45df-c43b-f3bace39d98c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "!pip install -U nltk\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "import os\n",
        "\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer\n",
        "from nltk import cluster\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.cluster import euclidean_distance\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "import pandas\n",
        "\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "from sklearn.cluster import SpectralClustering, KMeans, AgglomerativeClustering\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "from gensim.models import Word2Vec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3QnS7DkFqnR",
        "outputId": "646aeb09-49ad-4196-8aa7-372f8bfee098"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize average accuracy list\n",
        "scores = []\n",
        "for i in range(10):\n",
        "  path = \"/content/drive/MyDrive/product_reviews\"\n",
        "\n",
        "  sentences_all = []\n",
        "  sentences_clean = []\n",
        "  files = []\n",
        "\n",
        "  # generate stopwords\n",
        "  stops = Counter(stopwords.words())\n",
        "\n",
        "  # initialize stemmer\n",
        "\n",
        "  ps = PorterStemmer()\n",
        "  for p in os.listdir(path):\n",
        "    with open(path +\"/\" +p, \"r\") as f:\n",
        "      if p != \"README.txt\":\n",
        "        files.append(p)\n",
        "        text = f.read()\n",
        "        # preprocessing- remove all tags, non-alphanum chars, redundant spacing\n",
        "        text = re.sub(\"##\", \"\", text)\n",
        "        text = re.sub(\"\\[t\\]\", \"\", text)\n",
        "        text = re.sub(\"\\\\n\", \"\", text)\n",
        "        text = re.sub(\"\\[u\\]\", \"\", text)\n",
        "        text = re.sub(\"\\[(\\+|\\-)\\d\\]\", \" \", text)\n",
        "        text = text.lower()\n",
        "        sentences = text.split(\".\")\n",
        "        sentences = [re.sub(\"[^A-Za-z0-9 ]+\", \"\", sent) for sent in sentences]\n",
        "        sentences = [re.sub(\"\\s\\s+\", \" \", sent) for sent in sentences]\n",
        "\n",
        "        # remove stopwords, tokenize each sentence, stem each word in sentence\n",
        "        for sent in sentences:\n",
        "          sent = \" \".join([word for word in sent.split() if word not in stops])\n",
        "          tokens = word_tokenize(sent)\n",
        "          sentences_clean.append([ps.stem(t) for t in tokens])\n",
        "\n",
        "        sentences_all += sentences\n",
        "\n",
        "  # flatten sentences, remove empty sentnences\n",
        "  sentences_clean = [sentence for sentence in sentences_clean if sentence != []]\n",
        "\n",
        "  # remove very short words\n",
        "  flat_tokens = [word for sentence in sentences_clean for word in sentence if len(word) > 2]\n",
        "\n",
        "  # get most common words\n",
        "  fdist = FreqDist(flat_tokens)\n",
        "  targets = [t[0] for t in fdist.most_common(50)]\n",
        "\n",
        "  pseudos = []\n",
        "\n",
        "  for target in fdist.most_common(50): \n",
        "    # for each common word, generate its reverse, append this to list of reversed words\n",
        "    target_token = target[0]\n",
        "    freq = target[1]\n",
        "    pseudo = target_token[::-1]\n",
        "    pseudos.append(pseudo)\n",
        "\n",
        "    # find locations in corpus of all instances of target word\n",
        "    target_locations = []\n",
        "    for sentence_index, sentence in enumerate(sentences_clean):\n",
        "      for token_index, token in enumerate(sentence):\n",
        "        if token == target_token:\n",
        "          target_locations.append((sentence_index, token_index))\n",
        "\n",
        "    substitutions = []\n",
        "\n",
        "    # replace half of those instances with the psuedo\n",
        "    for i in range(int(freq/2)):\n",
        "      substitution = numpy.random.randint(low=0, high=len(target_locations))\n",
        "      substitutions.append(substitution)\n",
        "      sentences_clean[target_locations[substitution][0]][target_locations[substitution][1]] = pseudo\n",
        "\n",
        "  terms = targets + pseudos\n",
        "  # generate word model based on clean vocabulary, increase learning rate\n",
        "  word_model = Word2Vec(min_count=1,window=3,vector_size=72,sample=6e-5, alpha=0.15, min_alpha=0.0007)\n",
        "  word_model.build_vocab(sentences_clean)\n",
        "  word_model.train(sentences_clean, total_examples=word_model.corpus_count, epochs=32, report_delay=1)\n",
        "\n",
        "  # make list of words of form [target0, psuedo0, target1, psuedo1, ... target49, psuedo49]\n",
        "  vecs = []\n",
        "  c_words = []\n",
        "  for target in targets:\n",
        "    c_words.append(target)\n",
        "    vecs.append(word_model.wv[target])\n",
        "    c_words.append(target[::-1])\n",
        "    vecs.append(word_model.wv[target[::-1]])\n",
        "\n",
        "  # apply kmeans clustering algorithm with 50 clusters, increase iteration sizes, get predictions after fitting\n",
        "  clusterer = KMeans(n_clusters = 50, max_iter=600, n_init=20).fit_predict(vecs)\n",
        "\n",
        "  # pair of words matches, then increase correct classification by one\n",
        "  cor = 0\n",
        "  for i in range(0,100,2):\n",
        "    c1 = clusterer[i]\n",
        "    c2 = clusterer[i+1]\n",
        "    if c1 == c2:\n",
        "      cor += 1\n",
        "  avg = (cor/50)*100\n",
        "  scores.append(avg)\n",
        "  \n",
        "print(\"average correct classification: \" + str(numpy.mean(scores)) + \"%\")\n",
        "print(\"standard deviation: \" +str(numpy.std(scores)))"
      ],
      "metadata": {
        "id": "-wOWsbAX7WF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ac65e6-5d17-4030-a135-1c48d3a545f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average correct classification: 78.0%\n",
            "standard deviation: 3.5777087639996634\n"
          ]
        }
      ]
    }
  ]
}