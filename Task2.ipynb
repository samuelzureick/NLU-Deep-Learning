{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PD95BxwjISNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1074e79d-4003-407c-f356-0c591dda79fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liadZJnNMLrW",
        "outputId": "4dbc9705-7017-4b28-eb3f-eeb0b10ee538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.9.0 in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.12)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (0.28.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (4.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (2.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.0) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.9.0) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.9.0 in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0\n",
        "!pip install keras==2.9.0\n",
        "!pip install -U gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk import * \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding\n",
        "from keras.models import load_model, Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TKOqszaWLxvh"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/product_reviews\"\n",
        "\n",
        "reviews = []\n",
        "files = []\n",
        "\n",
        "for p in os.listdir(path):\n",
        "  with open(path +\"/\" +p, \"r\") as f:\n",
        "    if p != \"README.txt\":\n",
        "      files.append(p)\n",
        "      reviews.append(f.read().split(\"[t]\"))\n",
        "\n",
        "reviews = [review for product in reviews for review in product]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BP9FzXL7rdy8"
      },
      "outputs": [],
      "source": [
        "vecs = []\n",
        "labels = []\n",
        "stops = Counter(stopwords.words())\n",
        "lm = WordNetLemmatizer()\n",
        "for index, review in enumerate(reviews):\n",
        "  # use regex to remove all unneccessary tags\n",
        "  reviews[index] = re.sub(\"##\", \"\", reviews[index])\n",
        "  reviews[index] = re.sub(\"\\\\n\", \"\", reviews[index])\n",
        "  reviews[index] = reviews[index].lower()\n",
        "  reviews[index] = re.sub(\"(\\[cs\\])\", \" \", reviews[index])\n",
        "  reviews[index] = re.sub(\"(\\[u\\])\", \" \", reviews[index])\n",
        "  reviews[index] = re.sub(\"(\\[t\\])\", \"\", reviews[index])\n",
        "  reviews[index] = re.sub(\"(\\[s\\])\", \" \", reviews[index])\n",
        "  reviews[index] = re.sub(\"(\\[cc\\])\", \" \", reviews[index])\n",
        "  reviews[index] = re.sub(\"(\\[p\\])\", \" \", reviews[index])\n",
        "\n",
        "  # only count those reviews that indicate sentiment\n",
        "  scores = [int(s[0][1:-1]) for s in re.findall(\"(\\[(\\+|\\-)\\d\\])\", reviews[index])]\n",
        "  if len(scores) != 0:\n",
        "    # remove tags, non-alphanum chars, and redundant spacing\n",
        "    reviews[index] = re.sub(\"(\\[(\\+|\\-)\\d\\])\", \" \", reviews[index])\n",
        "    reviews[index] = re.sub(\"[^A-Za-z0-9' ]+\", \" \", reviews[index])\n",
        "    reviews[index] = re.sub(\"\\s\\s+\", \" \", reviews[index])\n",
        "\n",
        "    # remove stopwords\n",
        "    reviews[index] = [word for word in reviews[index].split(\" \") if word not in stops]\n",
        "\n",
        "    # lemmatize each word\n",
        "    reviews[index] = [lm.lemmatize(word) for word in reviews[index]]\n",
        "\n",
        "    # reduce time steps by trimming off really long reviews\n",
        "    if len(reviews[index]) < 300:\n",
        "      # append cleaned review to feature vector list\n",
        "      vecs.append(reviews[index])\n",
        "      # assign class labels\n",
        "      if numpy.mean(scores) > 0:\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYTBP8ja9l5a",
        "outputId": "23b132ab-a536-4a48-e989-b45a374b9631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6249 - accuracy: 0.6833\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6796 - accuracy: 0.8833\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6382 - accuracy: 0.7167\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0387 - accuracy: 0.4833\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7577 - accuracy: 0.6333\n",
            "Mean accuracy: 0.6799999892711639\n",
            "Standard Deviation: 0.12927146361241207\n",
            "Mean MSE Loss: 0.7477992653846741\n"
          ]
        }
      ],
      "source": [
        "# initialize fold size given 5 folds, initialize performance metrics\n",
        "fold_size = int(len(vecs) / 5)\n",
        "loss = []\n",
        "accuracy = []\n",
        "\n",
        "# build and train word2vec encoding model\n",
        "word_model = Word2Vec(min_count=1,window=5,vector_size=32, sg=1)\n",
        "word_model.build_vocab(vecs)\n",
        "word_model.train(vecs, total_examples=word_model.corpus_count, epochs=7, report_delay=1)\n",
        "\n",
        "#average each word vector to create 1 dimensional sentence vector based on encoding\n",
        "sentence_embeddings = []\n",
        "for ind, sentence in enumerate(vecs):\n",
        "  embedding = np.array([np.mean(word_model.wv[word]) for word in sentence])\n",
        "  sentence_embeddings.append(embedding)\n",
        "\n",
        "for i in range(5):\n",
        "  # pad sequences so all sequences are same length\n",
        "  sequences = pad_sequences(sentence_embeddings, dtype=\"float32\").tolist()\n",
        "\n",
        "  # label weights generated based on ratio of pos to neg class\n",
        "  #weights = {0:1.55154639, 1:0.7377451}\n",
        "\n",
        "  # split data and labels into test and train based on current fold\n",
        "  vecs_test = sequences[i*fold_size:(i+1)*fold_size]\n",
        "  vecs_test = np.asarray(vecs_test).astype(\"float32\")\n",
        "  labels_test = labels[i*fold_size:(i+1)*fold_size]\n",
        "  vecs_train = sequences[:i*fold_size] + sequences[(i+1)*fold_size:]\n",
        "  vecs_train = np.asarray(vecs_train).astype(\"float32\")\n",
        "  labels_train = labels[:i*fold_size] + labels[(i+1)*fold_size:]\n",
        "\n",
        "\n",
        "  # reshape for proper lstm dimensionality\n",
        "  labels_train = np.asarray(labels_train).reshape((-1,1))\n",
        "  vecs_train = vecs_train.reshape((vecs_train.shape[0], vecs_train.shape[1],1))\n",
        "\n",
        "  # Definining binary classifier\n",
        "  # Start with LSTM layer of size 64 with dropout of .5 to reduce loss\n",
        "  # End with dense layer with sigmoid activation for binary classification\n",
        "  classifier = Sequential()\n",
        "  classifier.add(layers.LSTM(64, dropout=.3))\n",
        "  classifier.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  # compile model with binary_crossentropy loss because we doing binary classification\n",
        "  # set checkpoint to allow us to recover best model generated during training\n",
        "  classifier.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\",])\n",
        "  cp = ModelCheckpoint(\"1.hdf5\", monitor=\"accuracy\", verbose=0 ,save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
        "\n",
        "  # train model for 5 epochs with a validation split of .1, referencing previous callbacks, with preiously established class weights\n",
        "  classifier.fit(vecs_train, labels_train, epochs=5, verbose=0, validation_split=.1, callbacks = [cp])\n",
        "\n",
        "  # reshape testing data and labels\n",
        "  labels_test = np.asarray(labels_test).reshape((-1,1))\n",
        "  vecs_test = vecs_test.reshape(vecs_test.shape[0], vecs_test.shape[1], 1)\n",
        "  vecs_test = np.asarray(vecs_test).astype(\"float32\")\n",
        "\n",
        "  # load best model\n",
        "  classifier = load_model(\"1.hdf5\")\n",
        "  scores = classifier.evaluate(vecs_test, labels_test)\n",
        "  loss.append(scores[0])\n",
        "  accuracy.append(scores[1])\n",
        "  acc = scores[1]\n",
        "  \n",
        "\n",
        "print(\"Mean accuracy: \" +str(np.mean(accuracy)))\n",
        "print(\"Standard Deviation: \" +str(np.std(accuracy)))\n",
        "print(\"Mean MSE Loss: \" +str(np.mean(loss)))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}